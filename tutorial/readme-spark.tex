\MDNAME\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DO NOT MODIFY THIS FILE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Steps for Setting up the Spark on the Rasberry Pi Cluster}

Pre-requisite - Hadoop should be installed

hid-sp18-412 hid-sp18-410 hid-sp18-408 hid-sp18-406

Navigate to the path /home/hduser

\begin{lstlisting}
cd /home/hduser 
\end{lstlisting}

Begin the download with the following command

\begin{lstlisting}
wget http://apache.claz.org/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz
   
   
\end{lstlisting}

Unzip the tar file of the spark

\begin{lstlisting}
tar -xzf spark-2.3.0-bin-hadoop2.7.tgz
   
\end{lstlisting}

Move the spark that is extracted to the directory /opt/

\begin{lstlisting}
sudo  mv spark-2.3.0-bin-hadoop2.7 /opt/
   
\end{lstlisting}

Navigate to the directory /opt/spark-2.3.0-bin-hadoop2.7/conf

\begin{lstlisting}
cd /opt/spark-2.3.0-bin-hadoop2.7/conf
   
\end{lstlisting}

Copy the template from spark-env.sh.template to spark-env.sh

\begin{lstlisting}
cp spark-env.sh.template spark-env.sh
    
\end{lstlisting}

Open spark-env.sh then set the spark host and the worker memory.

\begin{lstlisting}
vi spark-env.sh

SPARK_MASTER_HOST = 169.254.24.132
SPARK_WORKER_MEMORY = 512m
\end{lstlisting}

